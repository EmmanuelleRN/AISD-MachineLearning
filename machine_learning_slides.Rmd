---
title: "~"
subtitle: " "
date: " "
output:
  xaringan::moon_reader:
    css: [default, my_css.css, metropolis-fonts]
    lib_dir: libs
    nature:
      ratio: '16:9'
      slideNumberFormat: '%current%'
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

class: class: center, middle


<!-- css: [default, duke-blue, hygge-duke] -->

```{r setup, echo=FALSE}
options(htmltools.dir.version = FALSE)
xaringanExtra::use_logo('www/channels4_profile-removebg-preview.png')
```

```{r include=FALSE}
library(dplyr)
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
```

```{css, echo = F}
.regression table {
  font-size: 12px;     
}

.dataTables_info{
  font-size: 10px;
}

.dataTables_paginate{
  font-size: 10px;
}

.dataTables_length{
  font-size: 10px;
}
```


# Machine Learning

---

# Machine Learning - Concepts

- Machine Learning (ML) is a subset of Artificial Intelligence (AI)
- Algorithms that can improve automatically through experience and by the use of data without being explicit programmed, reason why we say that the algorithms learn.
- With ML algorithms we can build a model to make predictions or decisions.
- Machine learning algorithms are used many different applications, for example:
  - Medicine
  - Email filtering
  - Speech recognition
  - Computer vision

<center><img src="https://backend.mile.cloud/upload/module/3ab28920802a8e64b800fdd2d22e7940.png" height="200"></img></center>

---

class: center, middle
background-image: url(www/machine-learning.png)
background-size: contain

---

class: center, middle

# Supervised Learning

---

# Supervised Learning

.pull-left[
**Supervised learning** is where you have input variables ($X$) and an output variable ($y$) and you use an algorithm to learn the mapping function from the input to the output.

<center> y = f(X) </center>

- It is the most common type of Machine Learning problem

- It is called **supervised** because we have the label that tell us the correct information, and we are going to be corrected if we predict wrong.

- Supervised learning can be grouped into two problems:
  - **Regression:** The output variable is a real number, for example, weight
  - **Clustering:** The output variable is a category, for example, disease and no disease 
]

.pull-right[
<img src="https://maplearn.readthedocs.io/en/latest/_images/classif_reg.png"></img>
]
---

# Steps

To solve this kind of problem we have some steps to follow:

1. Data collection
2. Exploratory Data Analysis (EDA)
3. Data Processing
4. Data Modelling

I am going to briefly explain each of these steps that will be clarified when we have the example.

---

# Data Collection

Data is very important for building a Machine learning model, as the model will learn from the dataset provided.

Collecting, **cleaning** and managing your data properly is a key factor for any ML project.

<center><p style="color:red">Garbage in, Garbage out</p></center>

# Exploratory Data Analysis

Understand the characteristics and distribution of the data to help us understand what kind of model and what type of learning we are dealing with.

---

# Data Preprocessing

**Data preprocessing** is a process of cleaning the data and enabling them in the right format so they can be consumed by the algorithm it consists of data cleaning, missing values handling, feature selection and feature engineering 

If the dataset is not good and if there are missing values, outliers, or the features are not presented in a corrected format, the ML model built from this data will probably be bad. 

Methods usually used: encoding, normalization, imputation, outliers and NaN rejections, feature selection

## Encoding

**Encoding** is a method to transform the categorical data into numbers before using it to fit the model. For example, if we have the categories "cat" and "dog" we can encode them to 0 and 1.

---

## Feature scaling

Some ML models need to be on the "same range" to fit the data properly without the addition of bias due to different scales. For example, if we have as features height as 150cm and salary as 40000, salary might be more important due to potentially being in the 1000's but that's not necessarily true.

Methods:
- **Min-max normalization:** rescales the range of features to the new range in [0,1]
$$X_{normalised} = \frac{X - min(X)}{max(X) - min(x)}$$
- **Standardisation (Z-score normalisation):** Rescale the feature in a new range with a zero mean and a standard deviation is 1
$$X_{normalised} = \frac{X - \bar{X}}{\sigma_X}$$

---

## Missing values 

When we have missing data, the data won't be available, i.e., we have missing information. In this case we can:
- *Remove* the missing data
  - If enough data available
  - If the missing data does not have a pattern, i.e., is random
- *Inputation*: input the missing data using some statistical method
  - Use the mean or median of the variable to input missing values
  - Apply some algorithm, for example, linear or logistic regression to determine the likely value

## Feature selection

Select the most relevant variables for the model. This can be used to:
- Simplify the model
- Reduce overfitting
- Reduce training time

---

# Data Modelling

## Split the data

Before applying the model we need to make sure that we have a dataset to train and evaluate the model and another one to test the accuracy of the model.

We need to have different datasets to train and test the model because we want the model to learn and be able to **generalise on unseen data**. If we use the same dataset for training and testing, we are "leaking" information to the test and we can't garantee that the model can generalise. Also, having this split help us with problems like **overfitting** and **underfitting**.

The splits of the data usually are:
- Training
- Validation
- Testing

We usually split the data into 80% for training, 10% for validation and 10% for testing.

---

### Training set

It is the set of data that is used to train and make the model learn the hidden features/patterns in the data.

It should be large enough so that the model can “learn” correctly. Most of the data is used for training.

### Validation Set

This set is used to validate our model performance during training. This validation process gives information that helps us tune the model’s hyperparameters and configurations accordingly. 
- It tell us if the training is moving in the right direction or not.

The main idea of splitting the dataset into a validation set is to prevent our model from **overfitting** i.e., the model becomes really good at classifying the samples in the training set but cannot generalize and make accurate classifications on the data it has not seen before. 

### Test set

It tests the model after completing the training by providing an unbiased final model performance metric in terms of accuracy, precision, etc. 
- It answers the question: "How well does the model perform?"

---

## Modelling and evaluating the model

There are various machine learning models that you can choose according to the objective, for example, Linear Regression, Support Vector Machine (SVM), Decision Tree, Random Forest, K-Nearest Neighbors, Neural Network, K-means, etc.

We will see some of them when going through the examples.

Once we train the model, we need to evaluate it. Depending of your supervised learning problem, we have different metrics:
- **Classification metrics:** F1 score, precision, recall, accuracy
- **Regression metrics:** Mean Absolute Error, Mean Squared Error (MSE), Root Mean Square Error (RMSE), $R^2$ metric.

## Prediction

When the best model is determined, it can be used to predict the new samples on the testing set.

---

class: center, middle

# Regression

---

# Regression

**Regression** is a type of **supervised** learning. One example of algorithm that we can use for this kid of problem is a **Linear Regression**.

- Yes, it is the same Linear Regression as previously seem
  -  In general, ML doesn’t make any assumptions but the user should make sure that the data meet the assumptions of the linear regression model if he/she wants to use it 
  
To explain how to solve a Regression problem, I am going to use the dataset *Medical insurance costs*. The data contains medical information and costs billed by health insurance companies. It contains 1338 rows of data and the  columns: age, gender, BMI, children, smoker, region as features and insurance charges as the label.

I am going to use the R libraries `tidyverse` and `tidymodels`. 
- `tidymodels` is a library for machine learning workflow

---

# Regression example - Data collection

The first step is to collect the data. The data is available at github and the link can be seen on the example.

```{r}
# Load libraries
library(tidyverse)
library(tidymodels)

# Load data sets
url <- 'https://raw.githubusercontent.com/stedy/Machine-Learning-with-R-datasets/master/insurance.csv'
insurance <- read_csv(url)
```

We can see some of the data by using `glimpse`. We can see that we have 1338 rows and 7 columns. Also, `sex`, `smoker` and `region` are characters that will be transformed to factors.

```{r}
glimpse(insurance)
```

---

# Regression example - EDA

I'll be using the package `dlookr` to help us with the EDA. We are displaying:
- na: Number of NA values
- mean, standard deviation and IQR 
- p00: min value; p25: 1st quantile; p50: median; p75: third quantile; p100: max value

```{r}
library(dlookr)
library(DT)
dlookr::describe(insurance, quantiles = c(0, 0.25, 0.5, 0.75, 1),
                 statistics = c("na", "mean", "sd", "IQR", "quantiles")) %>%
  DT::datatable(rownames = FALSE, options = list(dom = 't',scrollX = TRUE, scrollCollapse = TRUE)) %>% formatRound(-1)
```


---

# EDA - Sex

```{r}
insurance %>% 
  group_by(sex) %>% 
  describe(quantiles = c(0, 0.25, 0.5, 0.75, 1),
           statistics = c("mean", "sd", "IQR", "quantiles")) %>%
  DT::datatable(rownames = FALSE, options = list(autoWidth = TRUE, dom = 't',scrollX = TRUE)) %>% formatRound(-1)
```

---

# EDA - Smoker

```{r}
insurance %>% 
  group_by(smoker) %>% 
  describe(quantiles = c(0, 0.25, 0.5, 0.75, 1),
           statistics = c("mean", "sd", "IQR", "quantiles")) %>%
  DT::datatable(rownames = FALSE, options = list(autoWidth = TRUE, dom = 't',scrollX = TRUE, scrollY = "300px")) %>% formatRound(-1)
```

---

# EDA - Region

```{r}
insurance %>% 
  group_by(region) %>% 
  describe(quantiles = c(0, 0.25, 0.5, 0.75, 1),
           statistics = c("mean", "sd", "IQR", "quantiles")) %>%
  DT::datatable(rownames = FALSE, options = list(autoWidth = TRUE, scrollX = TRUE, scrollY = "300px")) %>% formatRound(-1)
```

---

# EDA - correlation

```{r fig.height=6}
correlate(insurance) %>%
  plot()
```

---

# Regression example - Feature engineering

We need to transform the characters values to factors.

```{r}
insurance <- insurance %>%
  mutate_if(is.character, as.factor)

glimpse(insurance)
```

Ordinary least squares is invariant to the scale of numerical variables, while methods such as lasso or ridge regression are not. 
- For invariant methods there is no real need for standardisation
- For non-invariant methods you should standardise. 

---

# Regression - Data Splitting

We will be using the `initial_split()` function to partition the data into training and test sets. The main arguments of the function are: `data` and `prop` that represents the training split proportion.
  - The split will be 80/20 for training/test.

After creating the partition, we need to apply the `training()` and `testing()` functions to have the partitions.

> Remember to always use `set.seed()` to ensure your results are reproducible.

```{r}
set.seed(88)

# create split object
insurance_split <- initial_split(insurance, prop = 0.8)

# Build training data set
insurance_train <- insurance_split %>% training()
cat("Insurance train dimension:", nrow(insurance_train), "rows and", ncol(insurance_train), "columns")

# Build testing data set
insurance_test <- insurance_split %>% testing()
cat("Insurance test dimension:", nrow(insurance_test), "rows and", ncol(insurance_test), "columns")
```

---

# Regression - Modelling

The next step in the process is to build a linear regression model to fit our training data.

For every model type, such as linear regression, there are numerous packages (or *engines*) that can be used.

For example, we can use the `lm()` function from base R or the `stan_glm()` function from the `rstanarm` package. Both of these functions will fit a linear regression model to our data with slightly different implementations.

The `parsnip` package from `tidymodels` acts like an aggregator across the various modeling engines within R. This makes it easy to implement machine learning algorithms from different R packages with one unifying syntax.

To specify a model object with parsnip, we must:

1. Pick a model type
2. Set the engine
3. Set the mode (either regression or classification)

---

# Regression - Modelling

Linear regression is implemented with the `linear_reg()` function in `parsnip`. To the set the engine and mode, we use `set_engine()` and `set_mode()`.

Let’s create a linear regression model object with the `lm` engine. 
- This is the default engine for most applications.

```{r}
lm_model <- linear_reg() %>% 
            set_engine('lm') %>% # adds lm implementation of linear regression
            set_mode('regression')

# View object properties
lm_model
```

---

# Regression - Fit model to data

We can fit the model by using the `fit()` function from `parsnip`. The function has the following arguments:
- parnsip model 
- model formula
- data frame with the training data

In our formula, we have specified that `charges` is the response variable and age, sex, bmi, children, smoker and region are our predictor variables.

```{r}
lm_fit <- lm_model %>% 
          fit(charges ~ ., data = insurance_train)

# View lm_fit properties
lm_fit
```

---

# Regression - Exploring the model

To obtain the results from our trained model in a data frame, we can use the `tidy()` and `glance()` functions from the `broom` package.

The `tidy()` function takes a linear regression object and returns a data frame of the estimated model coefficients and their associated F-statistics and p-values.

```{r}
library(broom)
tidy(lm_fit) # Data frame of estimated coefficients
```

---

# Regression - Exploring the model

The `glance()` function returns performance metrics obtained on the training data such as the R2 value (r.squared) and the RMSE (sigma).

```{r}
# Performance metrics on training data
glance(lm_fit)
```

---

# Regression - Predict on the test set

To assess the performance of the model, we must use the test set to predict the charge value and then compare to the real value.

This is done with the `predict()` function. This function takes two important arguments:
- Trained model 
- new_data to generate predictions

It’s best to combine the test data set and the predictions into a single data frame.

```{r}
insurance_test_results <- predict(lm_fit, new_data = insurance_test) %>% 
                            bind_cols(insurance_test)

# View results
insurance_test_results %>% head()
```

---

# Regression - Evaluate the model

To evaluate the model, I'll use RMSE and $R^2$ that can be obtained by using the functions `rmse()` and `rsq()` functions. Both functions take the following arguments:
- `data`: dataframe with real and predicted values
- `truth`: column with the real labels
- `estimate`: columns with the predictions

```{r}
# RMSE on test set
rmse(insurance_test_results, truth = charges, estimate = .pred) %>%
  # R2 on the test set
  bind_rows(rsq(insurance_test_results, truth = charges, estimate = .pred))
```

The closes to zero the RMSE value is the best your model is, and we can see that the RMSE of this model is very high, which indicates that this model is not a very good model to predict insurance charges.

---

# Regression - Plot R2

.pull-left[

We can visualise the fit of the model by plotting the $R^2$.

```{r eval=FALSE, include=TRUE}
ggplot(data = insurance_test_results,
       mapping = aes(x = .pred, y = charges)) +
  geom_point(color = '#006EA1') +
  geom_abline(intercept = 0, 
              slope = 1, 
              color = 'orange') +
  labs(title = 'Linear Regression Results - 
       Insurance Test Set',
       x = 'Predicted Insurance Value',
       y = 'Actual Insurance Value')
```
]

.pull-right[
```{r echo=FALSE}
ggplot(data = insurance_test_results,
       mapping = aes(x = .pred, y = charges)) +
  geom_point(color = '#006EA1') +
  geom_abline(intercept = 0, slope = 1, color = 'orange') +
  labs(title = 'Linear Regression Results - Insurance Test Set',
       x = 'Predicted Insurance Value',
       y = 'Actual Insurance Value')
```
]

---

class: center, middle

# Classification

---

# Classification

**Classification** is a type of **supervised** learning where we categorise data into classes. There are many different algorithms that can help us solve this kind of problems.

Classification requires a training dataset with many examples of inputs and outputs from which to learn. It can be categorised in two types of problems:
  - **Binary classification:** The outcome has only **two** labels, for example, disease and not disease. 
    - Some popular algorithms are: Logistic Regression, Decision Tree, K-Nearest Neighbour (KNN)
  - **Multi-label classification:** The outcome has multiple labels, for example, dog, cat, bird and other.
    - Some popular algorithms: KNN, Decision Tree, Random Forest, Naive Bayes

To evaluate the model performance we can make use of ROC, confusion matrix, etc. We need to be aware of **class imbalance** problems.


---

# Evaluation metrics

.pull-left[
When thinking about classification, some common metrics to understand performance involves:
  - Accuracy
  - Precision
  - Recall
  - ROC
  
Most of this metrics can be determined by creating a **confusion matrix**, a contingency table with two dimensions (*actual* and *predicted*) that help us determine the performance of a classification model when we know the true classification of the data.
]

.pull-right[

<img src="https://www.nbshare.io/static/snapshots/cm_colored_1-min.png"></img>

]

---

# Evaluation metrics - Confusion Matrix

From the confusion matrix, we can get the following performance metrics:

- **Accuracy:** Proportion of correct predictions among the total number of cases examined.
$$\text{Acc} = \frac{TP + TN}{N + P}$$
- **Precision or Positive Predictive Value (PPV):** Proportion of positive predictions that was actually correct
$$\text{Pr} = \frac{TP}{TP + FP}$$

- **Sensitivity or recall or True Positive Rate (TPR):** Proportion of correct predictions among the positive instances.
$$\text{Recall} = \frac{TP}{TP + FN}$$
- **F1 Score:** Harmonic mean of precision and recall.
$$F_1 = \frac{2 \times TP}{2 \times TP + FP + FN}$$

---

# Evaluation metric - ROC curve

.pull-left[

**ROC curve** is a performance measurement for the classification problems at various threshold settings. This curve plots two parameters:
- True Positive Rate
- False Positive Rate = 1 - Specificity

From the ROC curve we can get the **Area Under the Curve (AUC)** metric that ranges from 0 to 1. A model whose predictions are 100% wrong has an AUC of 0.0; one whose predictions are 100% correct has an AUC of 1.0.
- When AUC is 0.5, it means the model has no class separation capacity whatsoever. 

]

.pull-right[

<img src="https://upload.wikimedia.org/wikipedia/commons/thumb/1/13/Roc_curve.svg/2048px-Roc_curve.svg.png"></img>

]

---

# Class Imbalance problem

An **imbalanced classification** problem is a problem where the distribution of examples across the known classes is biased or skewed. 
- The distribution can vary from a slight bias to a severe imbalance.

Most of the machine learning algorithms used for classification were designed around the assumption of an equal number of examples for each class. 
- This results in models that have poor predictive performance, specifically for the minority class
- Typically, the minority class is more important

For example, we want to detect fraudulent transaction, and if every 1000 transactions only 1 is fraudulent, it is very easy for the model to classify everything as non-fraud, so the accuracy of the model will be almost 1, but the actual class that we wanted to determine, it was wrongfully classified.
  - We need to look at other metrics that can tell us how the prediction performed for different classes
  
> Using **accuracy** as a peformance metric is **not recommended**.

We need to use techniques that will help us undersampling the majority class or oversampling the minority class, for example, Random undersampling and SMOTE oversampling.

---

# Classification - Example

The dataset we are going to use to perform the classification problem is `BreastCancer: Wisconsin Breast Cancer Database`. The data can be downloaded from the `mlbench` package.

The data has 699 observations on 11 variables, one being a character variable, 9 being ordered or nominal, and 1 target class.

```{r}
library(mlbench)
data(BreastCancer)
BreastCancer <- BreastCancer %>% select(-Id)
glimpse(BreastCancer)
```

---

# Classification example - Summary

```{r}
summary(BreastCancer)
BreastCancer %>% group_by(Class) %>% count() %>% ungroup() %>% mutate(prop = n / sum(n))
```

---

# Classification - Data Preprocessing

As we have missing data, we will drop the rows that are incomplete.

```{r}
BreastCancer %>% anyNA()
cat("Number of rows before dropping missing values: ", nrow(BreastCancer))
BreastCancer <- BreastCancer %>% drop_na()
cat("Number of rows after dropping missing values: ", nrow(BreastCancer))
```


As all the variables of the dataset, except Class, can be numerical features, I'll change the variable type.

```{r}
BreastCancer <- BreastCancer %>% 
  mutate_at(vars(-("Class")), as.numeric)
```

---

# Classification - Data Splitting

The split will be 80/20 for training/test.

```{r}
set.seed(88)

# create split object
bcancer_split <- initial_split(BreastCancer, prop = 0.8)

# Build training data set
bcancer_train <- bcancer_split %>% training()
cat("Breast cancer train dimension:", nrow(bcancer_train), "rows and", ncol(bcancer_train), "columns")

# Build testing data set
bcancer_test <- bcancer_split %>% testing()
cat("Breast cancer test dimension:", nrow(bcancer_test), "rows and", ncol(bcancer_test), "columns")
```

---

# Classification - Modelling

I will be using and comparing the results of two models:
  - Decision Tree
  - Suport Vector Machine (SVM)

.pull-left[
<img src="https://www.tutorialandexample.com/wp-content/uploads/2019/10/Decision-Trees-Root-Node.png"></img>
]

.pull-right[
<img src="https://miro.medium.com/max/1088/1*6U9NrruycDBsPOyivpn8UQ.png" height="300"></img>
]

---

# Classification - Modelling

Decision Tree and SVM are implemented with the  `decision_tree()` and `svm_linear()` functions in `parsnip`. To the set the engine and mode, we use `set_engine()` and `set_mode()`.

Let’s create this models using the following engines:
- `rpart` for the decision tree that needs the `rpart` package installed
- `LiblineaR` for the SVM that needs to have the package `LiblineaR` installed

```{r}
tree_model <- decision_tree() %>%
              set_engine('rpart') %>%
              set_mode('classification')

svm_model <- svm_linear() %>%
             set_engine('LiblineaR') %>%
             set_mode('classification')
```

---

# Classification - Fit model to data

`Class` is the response variable and all the other variables are our predictor variables.

```{r}
tree_fit <- tree_model %>% fit(Class ~ ., data = bcancer_train)
svm_fit <- svm_model %>% fit(Class ~ ., data = bcancer_train)

# View fit properties
svm_fit %>% broom::tidy()
```

---

# Classification - Predict on the test set

To assess the performance of the model, we must use the test set to predict the charge value and then compare to the real value.


```{r}
bcancer_test_results_tree <- predict(tree_fit, new_data = bcancer_test) %>% 
                             bind_cols(bcancer_test)
bcancer_test_results_tree %>% head(2)

bcancer_test_results_svm <- predict(svm_fit, new_data = bcancer_test) %>% 
                            bind_cols(bcancer_test)
bcancer_test_results_svm %>% head(2)                                      

```

---

# Classification - Evaluate the models

To evaluate the models, we will use the `confusionMatrix` function from `caret`.

.pull-left[
- Decision Tree
```{r}
library(caret)
confusionMatrix(bcancer_test_results_tree$.pred_class,
                bcancer_test_results_tree$Class) %>% 
  tidy() %>%
  filter(term %in% c("precision", "recall", "specificity", "f1", "accuracy")) %>% 
  select(term, class, estimate)
```
]

.pull-right[
- SVM
```{r}
confusionMatrix(bcancer_test_results_svm$.pred_class,
                       bcancer_test_results_svm$Class) %>% 
  tidy() %>%
  filter(term %in% c("precision", "recall", "specificity", "f1", "accuracy")) %>% 
  select(term, class, estimate)
```

]

---

class: center, middle

# Unsupervised Learning

---

# Unsupervised Learning

.pull-left[

This type of problem is called **Unsupervised** because, unlike supervised learning, we don't have a label.
- The algorithm learns patterns from unlabeled data. 

Common tasks:
- **Clustering:** Technique that groups unlabeled data based on their similarities or differences. 
  - Example: KMeans
- **Dimensionality Reduction**: Reduce the number of features while also preserving the integrity of the dataset as much as possible. 
  - Example: Principla Components Analysis (PCA)
  
Some challenges can occur when it allows machine learning models to execute without any human intervention. For example:
- Computational complexity due to a high volume of training data
- Longer training times
- Higher risk of inaccurate results
]

.pull-right[
<img src="https://miro.medium.com/max/433/1*Iihw0V-r0raMMtcDTFGGQA.png"></img>

]

---

class: center, middle

# Clustering

---

# Clustering

Clustering can be considered the most important unsupervised learning problem.

It involves discovering groups in data. Unlike supervised learning, clustering algorithms only interpret the input data and find natural groups or clusters in feature space.

A cluster is a collection of objects which are *similar* between them and are *dissimilar* to the objects belonging to other clusters.

Given a set of points, with some distance between points, grouping the points into some number of clusters, such that
- **internal (within the cluster)** distances should be small i.e members of clusters are close/similar to each other.
- **external (intra-cluster)** distances should be large i.e. members of different clusters are dissimilar.

---

# Distance metrics

We need to define a **distance** or **proximity** metric for 2 points.
- How similar or dissimilar 2 points are

- **Similarity $s(x_i, x_k)$**: Large if $x_i, x_k$ are similar
- **Dissimilarity (or distance) $d(x_i, x_k)$**: Small if $x_i, x_k$ are similar

Examples:

- **Euclidean Distance: ** $d(\textbf{p}, \textbf{q}) = \sqrt{\sum_{i=1}^n (p_i-q_i)^2}$

- **Mahalanobis Distance: ** $d_M(\textbf{p}, \textbf{q}) = \sqrt{(\textbf{p}-\textbf{y})^TS^{-1}(\textbf{p}-\textbf{y})}$

- **Jaccard Distance: ** $J(A, B) = \frac{|A \cap B|}{|A \cup B|} = \frac{|A \cap B|}{|A| + |B| - |A \cap B|}$

A *good* proximity measure is **very** application dependent. 

---

# Clustering - Example

The steps to perform an unsupervised learning task are very similar to an supervised one, with the difference that we don't need to predict anything.

For the example, we will use the the built-in R data set `USArrests`, which contains statistics in arrests per 100,000 residents for assault, murder, and rape in each of the 50 US states in 1973. It includes also the percent of the population living in urban areas.

```{r}
library(cluster)    # clustering algorithms
library(factoextra) # clustering algorithms & visualization

us_arrests <- USArrests
glimpse(us_arrests)
```

---

# Clustering - Data Preprocessing

We are going to check and remove NA's if the exist in the data

```{r}
us_arrests %>% anyNA()
```

As we don’t want the clustering algorithm to depend to an arbitrary variable unit, we start by scaling/standardizing the data using the R function `scale`:

```{r}
us_arrests_scaled <- us_arrests %>%
  scale()

head(us_arrests_scaled)
```

---

# Clustering - Distance

We can compute and visualise the distance matrix using the functions `get_dist` and `fviz_dist` from the `factoextra` package. 

- `get_dist:` Computing a distance matrix between the rows of a data matrix. 
  - The default distance computed is the **Euclidean**
- `fviz_dist:` Visualise the distance matrix

```{r fig.height=4}
distance <- get_dist(us_arrests_scaled)
fviz_dist(distance)
```

---

# Clustering - K-Means

**K-means clustering** is the most commonly used unsupervised machine learning algorithm for partitioning a given data set into a set of $k$ groups determine by the analyst. In k-means clustering, each cluster is represented by its center (i.e, centroid) which corresponds to the mean of points assigned to the cluster.

## Determining the optimal number of clusters

As the number of clusters is user specified, we would like to use the optimal number of clusters. The three most popular methods for determining the optimal clusters are:

- **Elbow method: ** Minimise the total intra-cluster variation (known as total within-cluster variation or total within-cluster sum of square)
  - Can be calculated using the function `fviz_nbclust` using method `wss`
- **Silhouette method: ** Measures the quality of a clustering, i.e., it determines how well each object lies within its cluster. A high average silhouette width indicates a good clustering.
  - Can be calculated using the function `fviz_nbclust` using method `silhouette`
- **Gap statistic: ** The approach can be applied to any clustering method. The gap statistic compares the total intracluster variation for different values of k with their expected values under null reference distribution of the data (i.e. a distribution with no obvious clustering).
  - Can be visulaised using the function `fviz_gap_stat` using the statistic generated by `clusGap`

---

# Clustering - Optimal number of clusters

```{r fig.height=4}
library(patchwork)

set.seed(123)

elbow_method <- fviz_nbclust(us_arrests_scaled, kmeans, method = "wss")
sill <- fviz_nbclust(us_arrests_scaled, kmeans, method = "silhouette")
gap_stat <- clusGap(us_arrests_scaled, FUN = kmeans, nstart = 25, K.max = 10, B = 50)
gap_plot <- fviz_gap_stat(gap_stat)

elbow_method + sill + gap_plot
```


---

# Clustering - KMeans

With most of these approaches suggesting 3 as the number of optimal clusters, we can perform the final analysis and extract the results using 3 clusters.

```{r}
# Compute k-means clustering with k = 3
set.seed(123)
kmeans <- kmeans(us_arrests_scaled, 3, nstart = 25)
print(kmeans)
```

---

# Clustering - Visualise clusters

We can visualise the clusters using the function `fviz_cluster`.

```{r fig.height=6}
fviz_cluster(kmeans, data = us_arrests_scaled)
```

---

# Clustering - Final step

And we can extract the clusters and add to our initial data to do some descriptive statistics at the cluster level:

```{r}
us_arrests %>%
  mutate(cluster = kmeans$cluster) %>%
  group_by(cluster) %>%
  summarise_all("mean")
```

---

class: center, middle

# Principal Components Analysis

---

# Principal Components Analysis (PCA)

**Principal Components Analysis (PCA)** is an unsupervised machine learning technique that seeks to find *principal components*, i.e., linear combinations of the original predictors, that explain a large portion of the variation in a dataset.
- The goal of PCA is to explain most of the variability in a dataset with fewer variables than the original dataset.

For the example we will use the dataset `fat` available in the `faraway` library. The dataset contains 18 physical measurements.

```{r}
library(faraway)
attach(fat)
glimpse(fat)
```

---

# PCA - Preprocessing

PCA is **influenced** by the magnitude of each variable; therefore, the results obtained when we perform PCA will also depend on whether the variables have been individually scaled.

```{r}
apply(fat, 2, var)
```

PCA works best with numerical data, so in case you have categorical features on the dataset, you need to "dummyfy" the features.

---

# PCA - Example

We can apply PCA with the function `prcomp`. You will also set two arguments, center and scale, to be TRUE. 

```{r}
fat_pca <- prcomp(fat, center = TRUE, scale. = TRUE)
summary(fat_pca)
```

You obtain 18 principal components. Each of these explains a percentage of the total variation in the dataset. 
-  PC1 explains 59.8% of the total variance, which means that almost half of the information in the dataset can be encapsulated by just that one Principal Component. 
- PC2 explains 15% of the variance. With just PC1 and PC2 we can explain ~74% of the variance.

---

# PCA - Plotting

We can plot PCA using the `ggbiplot` library. We need to install this library from github using the function `install_github` from `devtools` giving the github link as parameter of the function.

```{r fig.height=5}
#devtools::install_github("vqv/ggbiplot")
library(ggbiplot)
ggbiplot(fat_pca)
```

---

# Reference

https://github.com/stedy/Machine-Learning-with-R-datasets/blob/master/insurance.csv

The Elements of Statistical Learning

Machine Learning: A Probabilistic Perspective
